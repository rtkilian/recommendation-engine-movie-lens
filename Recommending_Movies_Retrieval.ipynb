{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommending Movies - Retrieval.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMabauyi1M5CMyZY2NWN7+K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rtkilian/recommendation-engine-movie-lens/blob/main/Recommending_Movies_Retrieval.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3r2ELNpxja1v"
      },
      "source": [
        "# Recommending movies: retrieval\r\n",
        "Real-world recommender systems are often made up of two tasks:\r\n",
        "1. Retrieval: select an initial set of hundreds of candidates from all possible candidates. This needs to be computationally efficient.\r\n",
        "2. Ranking: takes the output of the retrieval model and fine-tunes them to select only the best. \r\n",
        "\r\n",
        "Retrieval models are often composed of two sub-models:\r\n",
        "1. Query model: computes the query representation (normally a fixed-dimensionality embedding vector) using query features.\r\n",
        "2. Candidate model: computes the candidate representation (an equally-sized vector) using the candidate features\r\n",
        "\r\n",
        "The outputs of the two models are then multiplied together to give a query-candidate affinity score, with higher scores expressing a better match between the candidate and the query.\r\n",
        "\r\n",
        "In this notebook, I am going to build a two-tower model using the Movielens dataset. I will:\r\n",
        "1. Get the data and split into a training and test set.\r\n",
        "2. Implement a retrieval model.\r\n",
        "3. Fit and evaluate the model.\r\n",
        "4. Export the model for efficient serving by building an approximate nearest neighbours (ANN) index."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oH9KhX-lJ26"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RYtmlTlM5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90b9a0d-4685-414c-d0d6-a87d3367bf25"
      },
      "source": [
        "!pip install -q numpy==1.18.5 # we have to downgrade otherwise we get an error\r\n",
        "\r\n",
        "!pip install -q tensorflow-recommenders\r\n",
        "!pip install -q --upgrade tensorflow-datasets\r\n",
        "!pip install -q scann"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgY9V7-hlXJh",
        "outputId": "274bdc30-347c-4de8-92d8-f229db20e2ed"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "print(np.__version__)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.19.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvr-xBZxnf_N"
      },
      "source": [
        "import os\r\n",
        "import pprint\r\n",
        "import tempfile\r\n",
        "\r\n",
        "from typing import Dict, Text\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import tensorflow_datasets as tfds"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5DEUaQu-ntdX"
      },
      "source": [
        "import tensorflow_recommenders as tfrs"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlVdj7TLkuQz"
      },
      "source": [
        "## Data\r\n",
        "We can use the Movielens data in two ways:\r\n",
        "1. Explicitly: use the ratings from 1-5\r\n",
        "2. Implicitly: binary of 0 or 1, where 1=the user has watched the movie\r\n",
        "\r\n",
        "We are going to use the latter.\r\n",
        "\r\n",
        "We are going to use the data with 100k ratings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--_vp_zDktIZ"
      },
      "source": [
        "# Ratings data\r\n",
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\") # this data does not have any predefined splits\r\n",
        "\r\n",
        "# Features of all the available movies\r\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEnqFXAzOV71"
      },
      "source": [
        "The ratings dataset returns a dictionary of movie id, user id, the assigned rating, timestamp, movie information and user information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5iEhNHDL6XQ",
        "outputId": "5119bde1-8e3c-41f5-85d0-33b81ffc06c6"
      },
      "source": [
        "for x in ratings.take(1).as_numpy_iterator():\r\n",
        "  pprint.pprint(x)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'bucketized_user_age': 45.0,\n",
            " 'movie_genres': array([7]),\n",
            " 'movie_id': b'357',\n",
            " 'movie_title': b\"One Flew Over the Cuckoo's Nest (1975)\",\n",
            " 'raw_user_age': 46.0,\n",
            " 'timestamp': 879024327,\n",
            " 'user_gender': True,\n",
            " 'user_id': b'138',\n",
            " 'user_occupation_label': 4,\n",
            " 'user_occupation_text': b'doctor',\n",
            " 'user_rating': 4.0,\n",
            " 'user_zip_code': b'53211'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHCOs3igPiDL"
      },
      "source": [
        "The movies dataset contains the movie id, movie title, and data on what genres it belongs to. Note that the genres are encoded with integer labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ8yiB4WNbIP",
        "outputId": "632f033b-47e7-45c3-c15f-81c06dad5174"
      },
      "source": [
        "for x in movies.take(1).as_numpy_iterator():\r\n",
        "  pprint.pprint(x)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'movie_genres': array([4]),\n",
            " 'movie_id': b'1681',\n",
            " 'movie_title': b'You So Crazy (1994)'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3YU3lV-Pp6D"
      },
      "source": [
        "We are only going to keep the movie title and the user id in this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tXQOPFMQDfY"
      },
      "source": [
        "ratings = ratings.map(lambda x: {\r\n",
        "    \"movie_title\": x['movie_title'],\r\n",
        "    \"user_id\": x[\"user_id\"],\r\n",
        "})\r\n",
        "\r\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZqRO3YBRB9R"
      },
      "source": [
        "To fit and evaluate the model, we need to split it into a training and evaluation set. In an industrial recommender system, this would likely be done by time. The data up until a certain point would be used to predict the interactions after that point.\r\n",
        "\r\n",
        "However, for the purpose of this example, I am going to use an 80/20 split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpfWRzwnQd03"
      },
      "source": [
        "tf.random.set_seed(42)\r\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False) # shuffle the data\r\n",
        "\r\n",
        "train = shuffled.take(80_000)\r\n",
        "test = shuffled.skip(80_000).take(20_000)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv1NxOniTDk2"
      },
      "source": [
        "I am also going to determine the unique user ids and movie titles present in the data.\r\n",
        "\r\n",
        "This is required as I need to be able to map the raw values of our categorical features to the embedded vectors in the models. To do this, I need a vocab that maps a raw feature value to an integer in a continuous range: this allows us to look up the corresponding embedding in our embedding tables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keTSbGQoS9l1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27aadbc1-905b-4f5e-d174-97423742d1c4"
      },
      "source": [
        "movie_titles = movies.batch(1_000) # combines consecutive elements into batches\r\n",
        "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\r\n",
        "\r\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\r\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))\r\n",
        "\r\n",
        "unique_movie_titles[:10]"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b\"'Til There Was You (1997)\", b'1-900 (1994)',\n",
              "       b'101 Dalmatians (1996)', b'12 Angry Men (1957)', b'187 (1997)',\n",
              "       b'2 Days in the Valley (1996)',\n",
              "       b'20,000 Leagues Under the Sea (1954)',\n",
              "       b'2001: A Space Odyssey (1968)',\n",
              "       b'3 Ninjas: High Noon At Mega Mountain (1998)',\n",
              "       b'39 Steps, The (1935)'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqaVdgtW3RFQ",
        "outputId": "3588cc7c-5773-4d5f-9772-15ac00c488cd"
      },
      "source": [
        "unique_user_ids[:10]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'1', b'10', b'100', b'101', b'102', b'103', b'104', b'105',\n",
              "       b'106', b'107'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpqTJxNf1l7y"
      },
      "source": [
        "## Modelling\r\n",
        "As a reminder, we are building a two-tower retrieval model: query-tower and candidate tower. These can be built independently and combined at the end.\r\n",
        "\r\n",
        "### Query tower\r\n",
        "i.e. given a user id, what movies should we recommend to them with the candidate tower (i.e. movie titles).\r\n",
        "\r\n",
        "The first step is to decide on the dimensionality of the query and candidate representations.\r\n",
        "\r\n",
        "More information can be found here: https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/#:~:text=Keras%20offers%20an%20Embedding%20layer,represented%20by%20a%20unique%20integer.&text=The%20Embedding%20layer%20is%20initialized,words%20in%20the%20training%20dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIPmuUHP2Gdy"
      },
      "source": [
        "embedding_dimension = 32"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnowT4DA2Lsz"
      },
      "source": [
        "Higher values may result in a more accurate model but it will also be slower to fit (i.e. more parameters to learn) and may be prone to overfitting.\r\n",
        "\r\n",
        "The second step is to define the model itself. I am going to take the user ids and convert them into integers. These will then be converted into our learnt embeddings. The user ids will form part of our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkOPlpPo2KmE"
      },
      "source": [
        "user_model = tf.keras.Sequential([\r\n",
        "                                  tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_user_ids, mask_token=None),\r\n",
        "                                  # I add an additional embedding to account for unknown tokens\r\n",
        "                                  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\r\n",
        "])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJVIWW2c3Zf-"
      },
      "source": [
        "[tf.keras.layers.experimental.preprocessing.StringLookup](https://www.tensorflow.org/guide/keras/preprocessing_layers): \r\n",
        "* Translates a set of arbitrary strings into an integer output via a table-based lookup, with optional out-of-vocabulary handling\r\n",
        "\r\n",
        "[tf.keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding):\r\n",
        "* Turns positive integers (indexes) into dense vectors of fixed size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niqQE7Z35cFI"
      },
      "source": [
        "A simple model like this corresponds exactly to a classic matrix factorisation approach. While defining a subclass of tf.keras.Model for this simple model might be overkill, it could easily be extended to an arbitrarily complex model using standard Keras components, as long as we return an embedding_dimension wide output at the end."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Z8Xvg0e-ce_"
      },
      "source": [
        "### Candidate Tower\r\n",
        "I will use the same approach for the movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmqm5K4B3zcs"
      },
      "source": [
        "movie_model = tf.keras.Sequential([\r\n",
        "                                   tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=unique_movie_titles, mask_token=None),\r\n",
        "                                   tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\r\n",
        "])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q90HL52r_GDa"
      },
      "source": [
        "### Metrics\r\n",
        "In the training data, we have positive (user, movie) pairs. To figure out how good the model is, we need to compute an affinity score that the model calculates for this pair to the scores of all the possible other candidates. If the score for the positive pair is higher than all the other candidates, the model is highly accurate. \r\n",
        "\r\n",
        "To do this, I will use the [`tfrs.metrics.FactorizedTopK`](https://www.tensorflow.org/recommenders/examples/basic_retrieval) metric. The metric has one required argument: the dataset of candidate embeddings that are used as implicit negatives for evaluation. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK7mm6wQ_Fax"
      },
      "source": [
        "metrics = tfrs.metrics.FactorizedTopK(\r\n",
        "    candidates=movies.batch(128).map(movie_model)\r\n",
        ")"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey1PXnvMBPdS"
      },
      "source": [
        "### Loss\r\n",
        "The next component is the loss used to train the model. TFRS has several loss layers and tasks to make this easy.\r\n",
        "\r\n",
        "In this instance, I'll make use of the [`Retrieval`](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/tasks/Retrieval) task object: a convenient wrapper that bundles together the loss functions and metrics computation.\r\n",
        "\r\n",
        "According to the documentation, the default loss function is categorical crossentropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dz5IM0YtBNzq"
      },
      "source": [
        "task = tfrs.tasks.Retrieval(\r\n",
        "    metrics=metrics\r\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW7OYX5gCv8w"
      },
      "source": [
        "The task itself is a Keras layer that takes the query and candidate embeddings as arguments, and returns the computed loss: I'll use this to implement the model's training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL5n8HLcKBW4"
      },
      "source": [
        "### The Full Model\r\n",
        "I wil now combine it all into a single model. TFRS exposes a base model class [`tfrs.models.Model`](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/models/Model) which streamlines building models. Here, all I need to do is to set up the components of the `__init__` method, and implement the `compute_loss` method, taking in the raw features and returning the loss value.\r\n",
        "\r\n",
        "The base model will then take care of creating the appropriate trianing loop to fit our model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsmPEsf5Cu2Z"
      },
      "source": [
        "class MovielensModel(tfrs.Model):\r\n",
        "\r\n",
        "  def __init__(self, user_model, movie_model):\r\n",
        "    super().__init__()\r\n",
        "    self.movie_model: tf.keras.Model = movie_model\r\n",
        "    self.user_model: tf.keras.Model = user_model\r\n",
        "    self.task: tf.keras.layers.Layer = task\r\n",
        "\r\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\r\n",
        "    # We pick out the user features and pass them into the user model.\r\n",
        "    user_embeddings = self.user_model(features[\"user_id\"])\r\n",
        "    # And pick out the movie features and pass them into the movie model,\r\n",
        "    # getting embeddings back.\r\n",
        "    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\r\n",
        "\r\n",
        "    # The task computes the loss and the metrics.\r\n",
        "    return self.task(user_embeddings, positive_movie_embeddings)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZu1q8u_X0GX"
      },
      "source": [
        "## Fitting and Evaluating\r\n",
        "After defining the model, I use the standard Keras fitting and evaluation routines to fit and evaluate the model.\r\n",
        "\r\n",
        "I will first instantiate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99I2DlBXXzPE"
      },
      "source": [
        "model = MovielensModel(user_model, movie_model)\r\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioF_pt1EYXj3"
      },
      "source": [
        "Then shuffle, batch and cache the training and evaluation data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4UhcADuYOE6"
      },
      "source": [
        "cached_train = train.shuffle(100_000).batch(8192).cache()\r\n",
        "cached_test = test.batch(4096).cache()"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIAz-4NkY2B7"
      },
      "source": [
        "Then train the model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYVmEjXPY0x4",
        "outputId": "6ca601b2-d382-4686-f7ff-ee2ee6737395"
      },
      "source": [
        "model.fit(cached_train, epochs=3)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['counter:0'] when minimizing the loss.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 22s 2s/step - factorized_top_k/top_1_categorical_accuracy: 1.7500e-04 - factorized_top_k/top_5_categorical_accuracy: 0.0026 - factorized_top_k/top_10_categorical_accuracy: 0.0068 - factorized_top_k/top_50_categorical_accuracy: 0.0656 - factorized_top_k/top_100_categorical_accuracy: 0.1417 - loss: 69858.7230 - regularization_loss: 0.0000e+00 - total_loss: 69858.7230\n",
            "Epoch 2/3\n",
            "10/10 [==============================] - 22s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0016 - factorized_top_k/top_5_categorical_accuracy: 0.0148 - factorized_top_k/top_10_categorical_accuracy: 0.0321 - factorized_top_k/top_50_categorical_accuracy: 0.1585 - factorized_top_k/top_100_categorical_accuracy: 0.2833 - loss: 67464.2450 - regularization_loss: 0.0000e+00 - total_loss: 67464.2450\n",
            "Epoch 3/3\n",
            "10/10 [==============================] - 22s 2s/step - factorized_top_k/top_1_categorical_accuracy: 0.0019 - factorized_top_k/top_5_categorical_accuracy: 0.0203 - factorized_top_k/top_10_categorical_accuracy: 0.0424 - factorized_top_k/top_50_categorical_accuracy: 0.1857 - factorized_top_k/top_100_categorical_accuracy: 0.3136 - loss: 66286.6172 - regularization_loss: 0.0000e+00 - total_loss: 66286.6172\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f968c173550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNsw1UVkZbWA"
      },
      "source": [
        "As the model trains, the loss is falling and a set of top-k retrieval metrics is updated. These tell me whether the true positives in the top-k retrieved items from the entire candidate set. For example, a top-5 categorical accuracy metrics of 0.2 would tell me, on average, the true positive is in the top 5 retrieved items 20% of the time.\r\n",
        "\r\n",
        "In this example, I am also calculating the metrics during training as well as evaluation. Because this can be quite slow with large candidate sets, it may be best to turn metric calculation off in training, and only run it in evaluation.\r\n",
        "\r\n",
        "Finally, we can evaluate our model on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55cb3vDfY8EE",
        "outputId": "1aab4416-01a7-4a19-8452-e5f43e32488d"
      },
      "source": [
        "model.evaluate(cached_test, return_dict=True)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 4s 817ms/step - factorized_top_k/top_1_categorical_accuracy: 0.0011 - factorized_top_k/top_5_categorical_accuracy: 0.0091 - factorized_top_k/top_10_categorical_accuracy: 0.0209 - factorized_top_k/top_50_categorical_accuracy: 0.1225 - factorized_top_k/top_100_categorical_accuracy: 0.2360 - loss: 31087.4801 - regularization_loss: 0.0000e+00 - total_loss: 31087.4801\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'factorized_top_k/top_100_categorical_accuracy': 0.23600000143051147,\n",
              " 'factorized_top_k/top_10_categorical_accuracy': 0.020899999886751175,\n",
              " 'factorized_top_k/top_1_categorical_accuracy': 0.0010999999940395355,\n",
              " 'factorized_top_k/top_50_categorical_accuracy': 0.12250000238418579,\n",
              " 'factorized_top_k/top_5_categorical_accuracy': 0.009100000374019146,\n",
              " 'loss': 28240.8515625,\n",
              " 'regularization_loss': 0,\n",
              " 'total_loss': 28240.8515625}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjT2XTaKcQ3I"
      },
      "source": [
        "Test set performance is much worse than training performance due to two factors:\r\n",
        "1. Overfitting - which can be improved by regularization or user and movie features to generalise better on unseen data\r\n",
        "2. Re-recommendation of users' already watched movies. These known-positive watches can crowd out test movies out of top K recommendations.\r\n",
        "\r\n",
        "The second phenomenon can be tackled be excluding previously seen movies from test recommendations. This approach is relatively common in the recommender systems literature but this isn't done in this notebook. If not recommending past movies is import, we should expect appropriately specified models to learn this behaviour from past user history and contextual information. Additionally, it is often appropriate to recommend the same item multiple times (e.g. evergreen TV series or a regularly purchased item)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTKx1oBlfyI5"
      },
      "source": [
        "## Making predictions\r\n",
        "Now I have my model I can start to make predictions with it. I can use the [`tfrs.layers.factorized_top_k.BruteForce`](https://www.tensorflow.org/recommenders/api_docs/python/tfrs/layers/factorized_top_k/BruteForce) layer to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w__RpRoaom6",
        "outputId": "9d7f4ce7-f619-464e-db61-05245edfba07"
      },
      "source": [
        "# Create a model that takes in raw query features, and\r\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\r\n",
        "# recommends movies out of the entire movies dataset\r\n",
        "index.index(movies.batch(100).map(model.movie_model), movies)\r\n",
        "\r\n",
        "# Get recommendations\r\n",
        "_, titles = index(tf.constant([\"42\"]))\r\n",
        "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recommendations for user 42: [b'Bridges of Madison County, The (1995)' b'Aristocats, The (1970)'\n",
            " b'Rudy (1993)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCgRdF8tg6rY",
        "outputId": "7d74d288-ea60-4fe5-cf2e-d6b0808d9674"
      },
      "source": [
        "# Get recommendations\r\n",
        "_, titles = index(tf.constant([\"22\"]))\r\n",
        "print(f\"Recommendations for user 22: {titles[0, :20]}\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recommendations for user 22: [b'Super Mario Bros. (1993)' b'Shadow, The (1994)'\n",
            " b'Private Benjamin (1980)' b'Clean Slate (1994)'\n",
            " b'Naked Gun 33 1/3: The Final Insult (1994)' b'Home Alone (1990)'\n",
            " b'Judge Dredd (1995)' b'Star Trek V: The Final Frontier (1989)'\n",
            " b'Star Trek: The Motion Picture (1979)' b'Real Genius (1985)']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjJcptaAhJAO"
      },
      "source": [
        "Alternatively, we can use an approximate retrieval index to speed up predictions. This will make it possible to efficiently surfact recommendations from sets of tens of millions of candidates.\r\n",
        "\r\n",
        "To do so, we can use the `scann` package. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nHiLo9LhBaY",
        "outputId": "1ea4c6e1-acee-4ad2-e023-754b96fa3e99"
      },
      "source": [
        "scann_index = tfrs.layers.factorized_top_k.ScaNN(model.user_model)\r\n",
        "scann_index.index(movies.batch(100).map(model.movie_model), movies)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow_recommenders.layers.factorized_top_k.ScaNN at 0x7f968225ddd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IWSEEB-fhsuL",
        "outputId": "416e5e52-833c-4567-809a-0d749af1f1ff"
      },
      "source": [
        "# Get recommendations\r\n",
        "_, titles = index(tf.constant([\"22\"]))\r\n",
        "print(f\"Recommendations for user 22: {titles[0, :20]}\")"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recommendations for user 22: [b'Super Mario Bros. (1993)' b'Shadow, The (1994)'\n",
            " b'Private Benjamin (1980)' b'Clean Slate (1994)'\n",
            " b'Naked Gun 33 1/3: The Final Insult (1994)' b'Home Alone (1990)'\n",
            " b'Judge Dredd (1995)' b'Star Trek V: The Final Frontier (1989)'\n",
            " b'Star Trek: The Motion Picture (1979)' b'Real Genius (1985)']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}